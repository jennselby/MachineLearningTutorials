{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "RecurrentNeuralNetworksGithub.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMDVeZ5GdGMt",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. Go to https://colab.research.google.com and choose the \\\"Upload\\\" option to upload this notebook file.\n",
        "1. In the Edit menu, choose \\\"Notebook Settings\\\" and then set the \\\"Hardware Accelerator\\\" dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJTBvKsDdGM9",
        "colab_type": "text"
      },
      "source": [
        "## Documentation/Sources\n",
        "* [Class Notes](https://jennselby.github.io/MachineLearningCourseNotes/#recurrent-neural-networks)\n",
        "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
        "* [https://keras.io/](https://keras.io/) Keras API documentation\n",
        "* [Keras recurrent tutorial](https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l71UJUcft2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upgrade tensorflow to tensorflow 2\n",
        "%tensorflow_version 2.x\n",
        "# display matplotlib plots\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIXtkRJUdGNK",
        "colab_type": "text"
      },
      "source": [
        "# Exercise Option 1: Tune an RNN on the IMDB classification problem\n",
        "\n",
        "## The IMDB Dataset\n",
        "The [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) consists of movie reviews (x_train) that have been marked as positive or negative (y_train). See the [Word Vectors Tutorial](https://github.com/jennselby/MachineLearningTutorials/blob/master/WordVectors.ipynb) for more details on the IMDB dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inuacm-gdGNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSaPRf3jdGN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(imdb_x_train, imdb_y_train), (imdb_x_test, imdb_y_test) = imdb.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39utI19rdGOT",
        "colab_type": "text"
      },
      "source": [
        "For a standard keras model, every input has to be the same length, so we need to set some length after which we will cutoff the rest of the review. (We will also need to pad the shorter reviews with zeros to make them the same length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eESTV_w7dGOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cutoff = 500\n",
        "imdb_x_train_padded = sequence.pad_sequences(imdb_x_train, maxlen=cutoff)\n",
        "imdb_x_test_padded = sequence.pad_sequences(imdb_x_test, maxlen=cutoff)\n",
        "\n",
        "# see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
        "imdb_index_offset = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4nDtLSbdGPP",
        "colab_type": "text"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OplPqsdGPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu6SSVP1dGPn",
        "colab_type": "text"
      },
      "source": [
        "Define our model.\n",
        "\n",
        "Unlike last time, when we used convolutional layers, we're going to use an LSTM, a special type of recurrent network.\n",
        "\n",
        "Using recurrent networks means that rather than seeing these reviews as one input happening all at once, with the convolutional layers taking into account which words are next to each other, we are going to see them as a sequence of inputs, with one word occurring at each timestep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w_53JdvdGPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb_lstm_model = Sequential()\n",
        "imdb_lstm_model.add(Embedding(input_dim=len(imdb.get_word_index()) + imdb_index_offset,\n",
        "                              output_dim=100,\n",
        "                              input_length=cutoff))\n",
        "# return_sequences tells the LSTM to output the full sequence, for use by the next LSTM layer. The final\n",
        "# LSTM layer should return only the output sequence, for use in the Dense output layer\n",
        "imdb_lstm_model.add(LSTM(units=32, return_sequences=True))\n",
        "imdb_lstm_model.add(LSTM(units=32))\n",
        "imdb_lstm_model.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
        "imdb_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j8hw_d4dGP3",
        "colab_type": "text"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C53sOms-dGP7",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb7a4eba-a3e7-486c-fc50-93fe542b37b1"
      },
      "source": [
        "# Train using GPU acceleration\n",
        "# (see https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ)\n",
        "device_name = test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "with device('/device:GPU:0'):\n",
        "  imdb_lstm_model.fit(imdb_x_train_padded, imdb_y_train, epochs=1, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "25000/25000 [==============================] - 501s 20ms/step - loss: 0.3990 - binary_accuracy: 0.8142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x110387d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttftUbMudGQX",
        "colab_type": "text"
      },
      "source": [
        "Assess the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDwZP1NjdGQd",
        "colab_type": "code",
        "colab": {},
        "outputId": "56016d99-71fd-47d0-90a4-39b4e8c08f28"
      },
      "source": [
        "with device('/device:GPU:0'):\n",
        "  imdb_lstm_scores = imdb_lstm_model.evaluate(imdb_x_test_padded, imdb_y_test)\n",
        "  print('loss: {} accuracy: {}'.format(*imdb_lstm_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 190s 8ms/step\n",
            "loss: 0.3043981187725067 accuracy: 0.87712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtvb4cLIdGQt",
        "colab_type": "text"
      },
      "source": [
        "# Exercise Option 1\n",
        "\n",
        "Experiment with different model configurations from the one above. Try other recurrent layers, different numbers of layers, change some of the defaults. See [Keras Recurrent Layers](https://keras.io/layers/recurrent/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uKemyLedGQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlwPhmgNdGQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqeUgpacdGRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V16EbrezdGRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60vEh8J3dGTr",
        "colab_type": "text"
      },
      "source": [
        "# Exercise Option 2: Set up your own RNN model for the Reuters Classification Problem\n",
        "\n",
        "Take the model from exercise 1 (imdb_lstm_model) and modify it to classify the [Reuters data](https://keras.io/datasets/#reuters-newswire-topics-classification).\n",
        "\n",
        "Think about what you are trying to predict in this case, and how you will have to change your model to deal with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH-JRrVBdGTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import reuters\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvV9zafmdGTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(reuters_x_train, reuters_y_train), (reuters_x_test, reuters_y_test) = reuters.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrSInAvydGT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wqhj_UcdGT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}