{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't gone through [Carl's Tutorial](https://github.com/carlshan/intro_to_machine_learning/blob/master/lessons/Reinforcement_Learning/RL_Tutorial.md), you may want to go through that and implement the hill-climbing policy before attempting this tutorial. I would not recommend implementing the policy gradient strategy unless you're interested in learning more about lower-level tensorflow. This tutorial uses the keras, like the others this semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code taken from [https://gist.github.com/EderSantana/c7222daa328f0e885093](https://gist.github.com/EderSantana/c7222daa328f0e885093)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object):\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        \"\"\"\n",
    "        Input: action and states\n",
    "        Ouput: new states and reward\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        if action == 0:  # left\n",
    "            action = -1\n",
    "        elif action == 1:  # stay\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1  # right\n",
    "        f0, f1, basket = state[0]\n",
    "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
    "        f0 += 1\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        out = out[np.newaxis]\n",
    "\n",
    "        assert len(out.shape) == 2\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state[0]\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1  # draw fruit\n",
    "        canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket\n",
    "        return canvas\n",
    "\n",
    "    def _get_reward(self):\n",
    "        fruit_row, fruit_col, basket = self.state[0]\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _is_over(self):\n",
    "        if self.state[0, 0] == self.grid_size-1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
    "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
    "        self.state = np.asarray([0, n, m])[np.newaxis]\n",
    "\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i+1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-26b3cb13a0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {:03d}/999 | Loss {:.4f} | Win count {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "epsilon = .1  # exploration\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "epoch = 1000\n",
    "max_memory = 500\n",
    "hidden_size = 100\n",
    "batch_size = 50\n",
    "grid_size = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(num_actions))\n",
    "model.compile(sgd(lr=.2), \"mse\")\n",
    "\n",
    "# If you want to continue training from a previous model, just uncomment the line bellow\n",
    "# model.load_weights(\"model.h5\")\n",
    "\n",
    "# Define environment/game\n",
    "env = Catch(grid_size)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "\n",
    "# Train\n",
    "win_cnt = 0\n",
    "for e in range(epoch):\n",
    "    loss = 0.\n",
    "    env.reset()\n",
    "    game_over = False\n",
    "    # get initial input\n",
    "    input_t = env.observe()\n",
    "\n",
    "    while not game_over:\n",
    "        input_tm1 = input_t\n",
    "        # get next action\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        input_t, reward, game_over = env.act(action)\n",
    "        if reward == 1:\n",
    "            win_cnt += 1\n",
    "\n",
    "        # store experience\n",
    "        exp_replay.remember([input_tm1, action, reward, input_t], game_over)\n",
    "\n",
    "        # adapt model\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "\n",
    "        loss += model.train_on_batch(inputs, targets)[0]\n",
    "    print(\"Epoch {:03d}/999 | Loss {:.4f} | Win count {}\".format(e, loss, win_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCoYOItsMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11486f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = 0\n",
    "for e in range(10):\n",
    "    loss = 0.\n",
    "    env.reset()\n",
    "    game_over = False\n",
    "    # get initial input\n",
    "    input_t = env.observe()\n",
    "\n",
    "    plt.imshow(input_t.reshape((grid_size,)*2),\n",
    "               interpolation='none', cmap='gray')\n",
    "    plt.savefig(\"%03d.png\" % c)\n",
    "    c += 1\n",
    "    while not game_over:\n",
    "        input_tm1 = input_t\n",
    "\n",
    "        # get next action\n",
    "        q = model.predict(input_tm1)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        input_t, reward, game_over = env.act(action)\n",
    "\n",
    "        plt.imshow(input_t.reshape((grid_size,)*2),\n",
    "                   interpolation='none', cmap='gray')\n",
    "        plt.savefig(\"%03d.png\" % c)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
