{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't gone through [Carl's Tutorial](https://github.com/carlshan/intro_to_machine_learning/blob/master/lessons/Reinforcement_Learning/RL_Tutorial.md), you may want to go through that and implement the hill-climbing policy before attempting this tutorial. I would not recommend implementing the policy gradient strategy unless you're interested in learning more about lower-level tensorflow. This tutorial uses the keras, like the others this semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code taken from [https://gist.github.com/EderSantana/c7222daa328f0e885093](https://gist.github.com/EderSantana/c7222daa328f0e885093)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "To be able to run the animation below, make sure you have the latest version of matplotlib, by running `pip3 install matplotlib --upgrade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object):\n",
    "    def __init__(self, grid_size=10):\n",
    "        '''\n",
    "        Input: grid_size (length of the side of the canvas)\n",
    "        \n",
    "        Initializes internal state.\n",
    "        '''\n",
    "        self.grid_size = grid_size\n",
    "        self.min_basket_center = 1\n",
    "        self.max_basket_center = self.grid_size-2\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        '''\n",
    "        Input: action (0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Moves basket according to action. Moves fruit down. Updates state to reflect these movements\n",
    "        '''\n",
    "        if action == 0:  # left\n",
    "            movement = -1\n",
    "        elif action == 1:  # stay\n",
    "            movement = 0\n",
    "        elif action == 2: # right\n",
    "            movement = 1\n",
    "        else:\n",
    "            raise Exception('Invalid action {}'.format(action))\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        # move the basket unless this would move it off the edge of the grid\n",
    "        new_basket_center = min(max(self.min_basket_center, basket_center + movement), self.max_basket_center)\n",
    "        # move fruit down\n",
    "        fruit_y += 1\n",
    "        out = np.asarray([fruit_x, fruit_y, new_basket_center])\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        '''\n",
    "        Returns a 2D numpy array with 1s (white squares) at the locations of the fruit and basket and\n",
    "        0s (black squares) everywhere else.\n",
    "        '''\n",
    "        im_size = (self.grid_size, self.grid_size)\n",
    "        canvas = np.zeros(im_size)\n",
    "        \n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        canvas[fruit_y, fruit_x] = 1  # draw fruit\n",
    "        canvas[-1, basket_center-1:basket_center + 2] = 1  # draw 3-pixel basket\n",
    "        return canvas\n",
    "\n",
    "    def _get_reward(self):\n",
    "        '''\n",
    "        Returns 1 if the fruit was caught, -1 if it was dropped, and 0 if it is still in the air.\n",
    "        '''\n",
    "        fruit_x, fruit_y, basket_center = self.state\n",
    "        if fruit_y == self.grid_size-1:\n",
    "            if abs(fruit_x - basket_center) <= 1:\n",
    "                return 1 # it caught the fruit\n",
    "            else:\n",
    "                return -1 # it dropped the fruit\n",
    "        else:\n",
    "            return 0 # the fruit is still in the air\n",
    "\n",
    "    def observe(self):\n",
    "        '''\n",
    "        Returns the current canvas, as a 1D array.\n",
    "        '''\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        '''\n",
    "        Input: action (0 for left, 1 for stay, 2 for right)\n",
    "        \n",
    "        Returns:\n",
    "            current canvas (as a 1D array)\n",
    "            reward received after this action\n",
    "            True if game is over and False otherwise\n",
    "        '''\n",
    "        self._update_state(action)\n",
    "        observation = self.observe()\n",
    "        reward = self._get_reward()\n",
    "        game_over = (reward != 0) # if the reward is zero, the fruit is still in the air\n",
    "        return observation, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Updates internal state\n",
    "            fruit in a random column in the top row\n",
    "            basket center in a random column\n",
    "        '''\n",
    "        fruit_x = random.randint(0, self.grid_size-1)\n",
    "        fruit_y = 0\n",
    "        basket_center = random.randint(self.min_basket_center, self.max_basket_center)\n",
    "        self.state = np.asarray([fruit_x, fruit_y, basket_center])\n",
    "\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''\n",
    "        Input:\n",
    "            states: [starting_observation, action_taken, reward_received, new_observation]\n",
    "            game_over: boolean\n",
    "        Add the states and game over to the internal memory array. If the array is longer than\n",
    "        self.max_memory, drop the oldest memory\n",
    "        '''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        '''\n",
    "        Randomly chooses batch_size memories, possibly repeating.\n",
    "        For each of these memories, updates the models current best guesses about the value of taking a\n",
    "            certain action from the starting state, based on the reward received and the model's current\n",
    "            estimate of how valuable the new state is.\n",
    "        '''\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1] # the number of possible actions\n",
    "        env_dim = self.memory[0][0][0].shape[1] # the number of pixels in the image\n",
    "        input_size = min(len_memory, batch_size)\n",
    "        inputs = np.zeros((input_size, env_dim))\n",
    "        targets = np.zeros((input_size, num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=input_size)):\n",
    "            starting_observation, action_taken, reward_received, new_observation = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            # Set the input to the state that was observed in the game before an action was taken\n",
    "            inputs[i:i+1] = starting_observation\n",
    "            \n",
    "            # Start with the model's current best guesses about the value of taking each action from this state\n",
    "            targets[i] = model.predict(starting_observation)[0]\n",
    "            \n",
    "            # Now we need to update the value of the action that was taken                      \n",
    "            if game_over: \n",
    "                # if the game is over, give the actual reward received\n",
    "                targets[i, action_taken] = reward_received\n",
    "            else:\n",
    "                # if the game is not over, give the reward received (always zero in this particular game)\n",
    "                # plus the maximum reward predicted for state we got to by taking this action (with a discount)\n",
    "                Q_sa = np.max(model.predict(new_observation)[0])\n",
    "                targets[i, action_taken] = reward_received + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model and initialize the environment objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "epsilon = .1  # probability of exploration (choosing a random action instead of the current best one)\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "max_memory = 500\n",
    "hidden_size = 100\n",
    "batch_size = 50\n",
    "grid_size = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(num_actions))\n",
    "model.compile(sgd(lr=.2), \"mse\")\n",
    "\n",
    "# Define environment/game\n",
    "env = Catch(grid_size)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0000/0000 | Loss 0.0205 | Catch count 0\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_episodes = 1\n",
    "catch_count = 0\n",
    "for episode in range(num_episodes):\n",
    "    loss = 0.\n",
    "    env.reset()\n",
    "    game_over = False\n",
    "    # get initial input\n",
    "    starting_observation = env.observe()\n",
    "\n",
    "    while not game_over:\n",
    "        # get next action\n",
    "        if np.random.rand() <= epsilon:\n",
    "            # epsilon of the time, we just choose randomly\n",
    "            action = np.random.randint(0, num_actions, size=1)\n",
    "        else:\n",
    "            # find which action the model currently thinks is best from this state\n",
    "            q = model.predict(starting_observation)\n",
    "            action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        new_observation, reward, game_over = env.act(action)\n",
    "        if reward == 1:\n",
    "            catch_count += 1\n",
    "\n",
    "        # store experience\n",
    "        exp_replay.remember([starting_observation, action, reward, new_observation], game_over)\n",
    "\n",
    "        # get data updated based on the stored experiences\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "\n",
    "        # train model on the updated data\n",
    "        loss += model.train_on_batch(inputs, targets)\n",
    "        \n",
    "        starting_observation = new_observation # for next time through the loop\n",
    "        \n",
    "    # Print update from this episode\n",
    "    print(\"Episode {:04d}/{:04d} | Loss {:.4f} | Catch count {}\".format(episode, num_episodes-1, loss, catch_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the model play some games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation code from \n",
    "# https://matplotlib.org/examples/animation/dynamic_image.html\n",
    "# https://stackoverflow.com/questions/35532498/animation-in-ipython-notebook/46878531#46878531\n",
    "fig = plt.figure()\n",
    "game_over = True\n",
    "num_games = 10\n",
    "image = plt.imshow(np.zeros((grid_size, grid_size)),interpolation='none', cmap='gray', animated=True, vmin=0, vmax=1)\n",
    "animation = None\n",
    "observation = None\n",
    "def animate(*args):\n",
    "    global game_over, num_games, animation, observation\n",
    "    if num_games >= 0 and game_over:\n",
    "        # reset for the next game\n",
    "        game_over = False\n",
    "        num_games -= 1\n",
    "        env.reset()\n",
    "        observation = env.observe()\n",
    "        image.set_data(observation.reshape((grid_size, grid_size)))\n",
    "    elif num_games >= 0:\n",
    "        # get next action in the current game\n",
    "        q = model.predict(observation)\n",
    "        action = np.argmax(q[0])\n",
    "        \n",
    "        # apply action, get rewards and new state\n",
    "        observation, reward, game_over = env.act(action)\n",
    "        \n",
    "        image.set_array(observation.reshape((grid_size, grid_size)))\n",
    "    else:\n",
    "        # We are done. This is a little gross, but I wasn't sure how else to stop the\n",
    "        # animation when this function says it is done, rather than after a set amount of frames\n",
    "        animation._stop()\n",
    "    return [image]\n",
    "    \n",
    "animation = matplotlib.animation.FuncAnimation(fig, animate, blit=True, repeat=True)\n",
    "IPython.display.HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Exercises:\n",
    "1. We've been looking at convolutional neural networks for image processing. Try changing the model to use convolutional layers. Does this seem to work any better?\n",
    "1. Change the code so that the basket is trying to avoid getting hit by the fruit.\n",
    "1. Change the code so the game keeps going until the basket misses the fruit. (To test that this is working, you may want to change the number of games in the animation to 1.) How should this change the rewards that the model gets?\n",
    "1. Change the game to something entirely different.\n",
    "1. Try changing how the training works, perhaps by requiring episodes to complete, and using more of a policy gradient strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
