{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. If you haven't already installed Python3, get it from [Python.org](https://www.python.org/downloads/)\n",
    "1. If you haven't already installed Jupyter Notebook, run `python3 -m pip install jupyter`\n",
    "1. In Terminal, cd to the folder in which you downloaded this file and run `jupyter notebook`. This should open up a page in your web browser that shows all of the files in the current directory, so that you can open this file. You will need to leave this Terminal window up and running and use a different one for the rest of the instructions.\n",
    "1. Install the Gensim word2vec Python implementation: `pip3 install --upgrade gensim`\n",
    "1. Get the trained model (1billion_word_vectors.zip) from me via airdrop or flashdrive and put it in the same folder as the ipynb file, the folder in which you are running the jupyter notebook command.\n",
    "1. Unzip the trained model file. You should now have three files in the folder (if zip created a new folder, mor:\n",
    "    * 1billion_word_vectors\n",
    "    * 1billion_word_vectors.syn1neg.npy\n",
    "    * 1billion_word_vectors.wv.syn0.npy\n",
    "1. If you didn't install keras last time, install it now\n",
    "    1. Install the tensorflow machine learning library by typing the following into Terminal:\n",
    "    `pip3 install --upgrade tensorflow`\n",
    "    1. Install the keras machine learning library by typing the following into Terminal:\n",
    "    `pip3 install keras`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Details -- Do Not Do This\n",
    "This took awhile, which is why I'm giving you the trained file rather than having you do this. But just in case you're curious, here is how to create the trained model file.\n",
    "1. Download the corpus of sentences from [http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz)\n",
    "1. Unzip and unarchive the file: `tar zxf 1-billion-word-language-modeling-benchmark-r13output.tar.gz` \n",
    "1. Run the following Python code:\n",
    "    ```\n",
    "    from gensim.models import word2vec\n",
    "    import os\n",
    "\n",
    "    corpus_dir = '1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled'\n",
    "    sentences = word2vec.PathLineSentences(corpus_dir)\n",
    "    model = word2vec.Word2Vec(sentences) # just use all of the default settings for now\n",
    "    model.save('1billion_word_vectors')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation/Sources\n",
    "* [https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html) for more information about how to use gensim word2vec in general\n",
    "* [https://codekansas.github.io/blog/2016/gensim.html](https://codekansas.github.io/blog/2016/gensim.html) for information about using it to create embedding layers for neural networks.\n",
    "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
    "* [https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) for using pre-trained embeddings with keras (though the syntax they use for the model layers is different than most other tutorials I've seen).\n",
    "* [https://keras.io/](https://keras.io/) Keras API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model file into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = word2vec.Word2Vec.load('1billion_word_vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not need to continue training the model, we can save memory by keeping the parts we need (the word vectors themselves) and getting rid of the rest of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = wv_model.wv\n",
    "del wv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of word vectors\n",
    "Now we can look at some of the relationships between different words.\n",
    "\n",
    "Like [the gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html), let's start with a famous example: king + woman - man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8407387733459473),\n",
       " ('monarch', 0.7541723251342773),\n",
       " ('prince', 0.7350202798843384),\n",
       " ('princess', 0.6969081163406372),\n",
       " ('empress', 0.677180290222168),\n",
       " ('sultan', 0.6649758815765381),\n",
       " ('Chakri', 0.6451101899147034),\n",
       " ('goddess', 0.6439394950866699),\n",
       " ('ruler', 0.6275453567504883),\n",
       " ('kings', 0.6273428201675415)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one does not work as well as I'd hoped, but it gets close. Maybe you can find a better example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('okapi', 0.7140712738037109),\n",
       " ('gibbon', 0.7034620046615601),\n",
       " ('koala', 0.6972028017044067),\n",
       " ('cub', 0.6907659769058228),\n",
       " ('tortoise', 0.6886162757873535),\n",
       " ('beetle', 0.685947597026825),\n",
       " ('salamander', 0.6855185627937317),\n",
       " ('psyllid', 0.6837549209594727),\n",
       " ('lynx', 0.6802860498428345),\n",
       " ('carnivore', 0.6794542670249939)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.most_similar(positive=['panda', 'eucalyptus'], negative=['bamboo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one of these is not like the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'laptop'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.doesnt_match(['red', 'purple', 'laptop', 'turquoise', 'ruby'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How far apart are different words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.205414  , 0.36557418, 0.65974367], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.distances('laptop', ['computer', 'phone', 'rabbit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what one of these vectors actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50756323, -2.8890731 ,  0.9743826 , -0.60089743, -0.23762947,\n",
       "       -2.324566  , -0.64634913, -0.66476715, -2.3432739 ,  1.4446437 ,\n",
       "       -0.15542823,  1.8248576 ,  1.1309539 , -0.21071543, -0.82512087,\n",
       "       -0.2773584 , -0.1973424 , -0.5337731 ,  2.1143918 ,  1.0673765 ,\n",
       "       -0.2341243 ,  1.5292411 ,  0.66977274,  1.1214821 , -0.57710004,\n",
       "       -0.02504024,  0.6074397 ,  0.19416903, -1.1265849 , -0.6618393 ,\n",
       "        1.7525213 ,  1.6232891 , -0.3886833 , -1.1867149 ,  0.45511633,\n",
       "        1.4240934 , -0.87929034, -1.8920534 ,  2.6986032 , -0.5277589 ,\n",
       "        2.1202435 ,  0.62670445,  1.0352231 ,  1.4998924 ,  2.5809426 ,\n",
       "        0.74698585, -0.07757699, -0.67074645,  1.6887746 , -0.22081567,\n",
       "        1.2107906 ,  0.16741815,  3.3496742 ,  1.1832954 ,  0.4423463 ,\n",
       "        0.04771314, -0.14557275, -1.3345221 ,  1.3236852 ,  2.0154989 ,\n",
       "       -0.6510446 ,  0.21808812, -0.31578887, -1.822629  ,  0.8436349 ,\n",
       "       -1.1500564 ,  1.24044   , -2.6430037 ,  1.0617311 ,  1.2009143 ,\n",
       "        2.910486  ,  0.7534945 , -0.74546903, -0.12999983,  0.62368834,\n",
       "       -0.34848922,  0.9471761 ,  2.3520417 ,  2.1063364 , -1.3959917 ,\n",
       "        0.01752609,  0.92311406,  0.3009809 , -1.680638  ,  0.47537982,\n",
       "       -1.3736504 , -0.19134097,  1.03852   , -1.8218307 ,  1.7354008 ,\n",
       "        0.6050655 , -3.3719819 ,  0.3659331 , -0.71509886, -1.7725863 ,\n",
       "       -3.8122861 ,  2.9105155 , -0.89803743,  1.2961383 , -0.88507044],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec['textbook']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other methods are available to us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on EuclideanKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class EuclideanKeyedVectors(KeyedVectorsBase)\n",
      " |  Class to contain vectors and vocab for the Word2Vec training class and other w2v methods not directly\n",
      " |  involved in training such as most_similar()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EuclideanKeyedVectors\n",
      " |      KeyedVectorsBase\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function EuclideanKeyedVectors.most_similar at 0x108009950>, case_insensitive=True)\n",
      " |      Compute accuracy of the model. `questions` is a filename where lines are\n",
      " |      4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |      See questions-words.txt in\n",
      " |      https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip\n",
      " |      for an example.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all questions containing a word not in the first `restrict_vocab`\n",
      " |      words (default 30,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      In case `case_insensitive` is True, the first `restrict_vocab` words are taken first, and then\n",
      " |      case normalization is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in questions and vocab to their uppercase form before\n",
      " |      evaluating the accuracy (default True). Useful in case of case-mismatch between training tokens\n",
      " |      and question words. In case of multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.distance('woman', 'man')\n",
      " |        0.34\n",
      " |      \n",
      " |        >>> trained_model.distance('woman', 'woman')\n",
      " |        0.0\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : str or numpy.array\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      \n",
      " |      other_words : iterable(str) or None\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`,\n",
      " |          in the same order as `other_words`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Raises KeyError if either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
      " |        'cereal'\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments. `pairs` is a filename of a dataset where\n",
      " |      lines are 3-tuples, each consisting of a word pair and a similarity value, separated by `delimiter`.\n",
      " |      An example dataset is included in Gensim (test/test_data/wordsim353.tsv). More datasets can be found at\n",
      " |      http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html or https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      The model is evaluated using Pearson correlation coefficient and Spearman rank-order correlation coefficient\n",
      " |      between the similarities from the dataset and the similarities produced by the model itself.\n",
      " |      The results are printed to log and returned as a triple (pearson, spearman, ratio of pairs with unknown words).\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all word pairs containing a word not in the first `restrict_vocab`\n",
      " |      words (default 300,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      If `case_insensitive` is True, the first `restrict_vocab` words are taken, and then case normalization\n",
      " |      is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in the pairs and vocab to their uppercase form before\n",
      " |      evaluating the model (default True). Useful when you expect case-mismatch between training tokens\n",
      " |      and words pairs in the dataset. If there are multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Use `dummy4unknown=True` to produce zero-valued similarities for pairs with out-of-vocabulary words.\n",
      " |      Otherwise (default False), these pairs are skipped entirely.\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False)\n",
      " |      Return a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      If `replace` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |      \n",
      " |      Note that you **cannot continue training** after doing a replace. The model becomes\n",
      " |      effectively read-only = you can call `most_similar`, `similarity` etc., but not `train`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words. Positive words contribute positively towards the\n",
      " |      similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      If topn is False, most_similar returns the vector of similarity scores.\n",
      " |      \n",
      " |      `restrict_vocab` is an optional integer which limits the range of vectors which\n",
      " |      are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |      only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |      meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
      " |        [('queen', 0.50882536), ...]\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective\n",
      " |      proposed by Omer Levy and Yoav Goldberg in [4]_. Positive words still contribute\n",
      " |      positively towards the similarity, negative words negatively, but with less\n",
      " |      susceptibility to one large distance dominating the calculation.\n",
      " |      \n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively â€“ a potentially sensible but untested extension of the method. (With\n",
      " |      a single positive example, rankings will be the same as in the default most_similar.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])\n",
      " |        [(u'iraq', 0.8488819003105164), ...]\n",
      " |      \n",
      " |      .. [4] Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n",
      " |        0.61540466561049689\n",
      " |      \n",
      " |        >>> trained_model.n_similarity(['restaurant', 'japanese'], ['japanese', 'restaurant'])\n",
      " |        1.0000000000000004\n",
      " |      \n",
      " |        >>> trained_model.n_similarity(['sushi'], ['restaurant']) == trained_model.similarity('sushi', 'restaurant')\n",
      " |        True\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the object to file (also see `load`).\n",
      " |      \n",
      " |      `fname_or_handle` is either a string specifying the file name to\n",
      " |      save to, or an open file-like object which can be written to. If\n",
      " |      the object is a file handle, no special array handling will be\n",
      " |      performed; all attributes will be saved to the same file.\n",
      " |      \n",
      " |      If `separately` is None, automatically detect large\n",
      " |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |      them into separate files. This avoids pickle memory errors and\n",
      " |      allows mmap'ing large arrays back on load efficiently.\n",
      " |      \n",
      " |      You can also set `separately` manually, in which case it must be\n",
      " |      a list of attribute names to be stored in separate files. The\n",
      " |      automatic check is not performed in this case.\n",
      " |      \n",
      " |      `ignore` is a set of attribute names to *not* serialize (file\n",
      " |      handles, caches etc). On subsequent load() these attributes will\n",
      " |      be set to None.\n",
      " |      \n",
      " |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      " |      in both Python 2 and 3.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      If topn is False, similar_by_vector returns the vector of similarity scores.\n",
      " |      \n",
      " |      `restrict_vocab` is an optional integer which limits the range of vectors which\n",
      " |      are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |      only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |      meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similar_by_vector([1,2])\n",
      " |        [('survey', 0.9942699074745178), ...]\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      If topn is False, similar_by_word returns the vector of similarity scores.\n",
      " |      \n",
      " |      `restrict_vocab` is an optional integer which limits the range of vectors which\n",
      " |      are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |      only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |      meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similar_by_word('graph')\n",
      " |        [('user', 0.9999163150787354), ...]\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similarity('woman', 'man')\n",
      " |        0.73723527\n",
      " |      \n",
      " |        >>> trained_model.similarity('woman', 'woman')\n",
      " |        1.0\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents. When using this\n",
      " |      code, please consider citing the following papers:\n",
      " |      \n",
      " |      .. Ofir Pele and Michael Werman, \"A linear time histogram metric for improved SIFT matching\".\n",
      " |      .. Ofir Pele and Michael Werman, \"Fast and robust earth mover's distances\".\n",
      " |      .. Matt Kusner et al. \"From Word Embeddings To Document Distances\".\n",
      " |      \n",
      " |      Note that if one of the documents have no words that exist in the\n",
      " |      Word2Vec vocab, `float('inf')` (i.e. infinity) will be returned.\n",
      " |      \n",
      " |      This method only works if `pyemd` is installed (can be installed via pip, but requires a C compiler).\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> # Train word2vec model.\n",
      " |          >>> model = Word2Vec(sentences)\n",
      " |      \n",
      " |          >>> # Some sentences to test.\n",
      " |          >>> sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
      " |          >>> sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
      " |      \n",
      " |          >>> # Remove their stopwords.\n",
      " |          >>> from nltk.corpus import stopwords\n",
      " |          >>> stopwords = nltk.corpus.stopwords.words('english')\n",
      " |          >>> sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
      " |          >>> sentence_president = [w for w in sentence_president if w not in stopwords]\n",
      " |      \n",
      " |          >>> # Compute WMD.\n",
      " |          >>> distance = model.wmdistance(sentence_obama, sentence_president)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Accept a single word as input.\n",
      " |      Returns the word's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      If `use_norm` is True, returns the normalized word vector.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model['office']\n",
      " |        array([ -1.40128313e-02, ...])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Return cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.array\n",
      " |          vector from which similarities are to be computed.\n",
      " |          expected shape (dim,)\n",
      " |      vectors_all : numpy.array\n",
      " |          for each row in vectors_all, distance from vector_1 is computed.\n",
      " |          expected shape (num_vectors, dim)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Contains cosine distance between vector_1 and each row in vectors_all.\n",
      " |          shape (num_vectors,)\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from KeyedVectorsBase:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Accept a single word or a list of words as input.\n",
      " |      \n",
      " |      If a single word: returns the word's representations in vector space, as\n",
      " |      a 1D numpy array.\n",
      " |      \n",
      " |      Multiple words: return the words' representations in vector space, as a\n",
      " |      2d numpy array: #words x #vector_size. Matrix rows are in the same order\n",
      " |      as in input.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model['office']\n",
      " |        array([ -1.40128313e-02, ...])\n",
      " |      \n",
      " |        >>> trained_model[['office', 'products']]\n",
      " |        array([ -1.40128313e-02, ...]\n",
      " |              [ -1.70425311e-03, ...]\n",
      " |               ...)\n",
      " |  \n",
      " |  most_similar_to_given(self, w1, word_list)\n",
      " |      Return the word from word_list most similar to w1.\n",
      " |      \n",
      " |      Args:\n",
      " |          w1 (str): a word\n",
      " |          word_list (list): list of words containing a word most similar to w1\n",
      " |      \n",
      " |      Returns:\n",
      " |          the word in word_list with the highest similarity to w1\n",
      " |      \n",
      " |      Raises:\n",
      " |          KeyError: If w1 or any word in word_list is not in the vocabulary\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
      " |        'sound'\n",
      " |      \n",
      " |        >>> trained_model.most_similar_to_given('snake', ['food', 'pencil', 'animal', 'phone'])\n",
      " |        'animal'\n",
      " |  \n",
      " |  rank(self, w1, w2)\n",
      " |      Rank of the distance of `w2` from `w1`, in relation to distances of all words from `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Rank of `w2` from `w1` in relation to all other nodes.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> model.rank('mammal.n.01', 'carnivore.n.01')\n",
      " |      3\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |       `fname` is the file used to save the vectors in\n",
      " |       `fvocab` is an optional file used to save the vocabulary\n",
      " |       `binary` is an optional boolean indicating whether the data is to be saved\n",
      " |       in binary word2vec format (default: False)\n",
      " |       `total_vec` is an optional parameter to explicitly specify total no. of vectors\n",
      " |       (in case word vectors are appended with document vectors afterwards)\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Returns all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> model.words_closer_than('carnivore.n.01', 'mammal.n.01')\n",
      " |      ['dog.n.01', 'canine.n.02']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from KeyedVectorsBase:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Note that the information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      `binary` is a boolean indicating whether the data is in binary word2vec format.\n",
      " |      `norm_only` is a boolean indicating whether to only store normalised word2vec vectors in memory.\n",
      " |      Word counts are read from `fvocab` filename, if set (this is the file generated\n",
      " |      by `-save-vocab` flag of the original C tool).\n",
      " |      \n",
      " |      If you trained the C model using non-utf8 encoding for words, specify that\n",
      " |      encoding in `encoding`.\n",
      " |      \n",
      " |      `unicode_errors`, default 'strict', is a string suitable to be passed as the `errors`\n",
      " |      argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |      file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |      (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      \n",
      " |      `limit` sets a maximum number of word-vectors to read from the file. The default,\n",
      " |      None, means read all.\n",
      " |      \n",
      " |      `datatype` (experimental) can coerce dimensions to a non-default float type (such\n",
      " |      as np.float16) to save memory. (Such types may result in much slower bulk operations\n",
      " |      or incompatibility with optimized routines.)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      If the object was saved with large arrays stored separately, you can load\n",
      " |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      " |      mmap, load large arrays as normal objects.\n",
      " |      \n",
      " |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      " |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      " |      is encountered.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wordvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Explore Word Vectors\n",
    "What other interesting relationship can you find, using the methods used in the examples above or anything you find in the help message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the word vectors in an embedding layer of a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed in the help text for wordvec that it has a built-in method for converting into a Keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_layer = wordvec.get_keras_embedding()\n",
    "test_embedding_layer.input_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Sequential()\n",
    "embedding_model.add(test_embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we actually use this? If you look at the [Keras Embedding Layer documentation](https://keras.io/layers/embeddings/) you might notice that it takes numerical input, not strings. How do we know which number corresponds to a particular word? In addition to having a vector, each word has an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.vocab['python'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get the same vector from the embedding layer as we get from our word vector object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1750487e+00,  2.3066440e-04, -6.0706180e-01, -1.1156354e+00,\n",
       "       -1.0580894e+00, -2.7154784e+00, -3.6140988e+00, -1.0810910e+00,\n",
       "        1.1234255e+00, -7.7326834e-01, -1.3322397e+00,  9.2905626e-02,\n",
       "       -2.4488842e+00, -1.7817341e-01, -3.5459950e+00, -1.7320968e+00,\n",
       "        1.9397168e+00, -6.3734710e-01,  2.3254216e+00, -1.3535864e+00,\n",
       "       -1.4451812e-01, -2.4297442e+00,  1.5498929e+00,  8.1969726e-01,\n",
       "        9.0982294e-01, -6.6116208e-01,  3.8905215e-01,  3.3855909e-01,\n",
       "       -7.5454485e-01, -1.0352553e+00, -2.5936973e+00,  1.2103225e+00,\n",
       "       -3.0236175e+00,  3.0580134e+00, -3.9140179e+00,  4.0223894e-01,\n",
       "        1.7356061e+00,  9.0976155e-01,  2.0956397e-02,  2.0190549e+00,\n",
       "        4.5332021e-01, -1.6634842e+00, -4.8180079e-01,  2.0414692e-01,\n",
       "       -5.9267312e-01, -1.4182589e+00, -9.7301149e-01,  5.1611459e-01,\n",
       "        2.0727324e+00,  2.0064230e+00, -7.5027935e-02, -1.1723986e+00,\n",
       "       -8.6943096e-01,  1.7028141e+00,  2.2190344e+00,  9.3605727e-01,\n",
       "       -2.3281140e+00, -1.1469719e+00, -7.4651584e-02, -1.3099817e+00,\n",
       "       -8.7831926e-01,  1.0963516e+00, -8.0801606e-01,  2.0202060e+00,\n",
       "       -2.8135295e+00, -1.5866054e+00,  1.7901597e+00, -1.9552002e+00,\n",
       "        8.5110778e-01, -4.1770697e+00,  5.4225093e-01, -1.9657525e+00,\n",
       "        2.0231185e+00, -1.8881788e+00,  5.9491843e-01,  8.3414179e-01,\n",
       "       -1.6628648e+00,  3.3324170e-01,  2.7011216e+00,  2.1156013e+00,\n",
       "        5.9008741e-01, -9.1122669e-01, -2.8129277e-01, -8.5378832e-01,\n",
       "        6.6145062e-01,  4.0669715e-01,  1.1161113e+00, -2.4905500e+00,\n",
       "        1.2948816e+00,  7.9231268e-01,  8.6634362e-01, -2.0348356e+00,\n",
       "        2.6159151e+00,  1.5450108e+00, -3.0899661e+00, -6.3572246e-01,\n",
       "       -2.5528045e+00,  3.4042710e-01, -2.1339917e+00, -3.5500580e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec['python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.1750487e+00,  2.3066440e-04, -6.0706180e-01, -1.1156354e+00,\n",
       "         -1.0580894e+00, -2.7154784e+00, -3.6140988e+00, -1.0810910e+00,\n",
       "          1.1234255e+00, -7.7326834e-01, -1.3322397e+00,  9.2905626e-02,\n",
       "         -2.4488842e+00, -1.7817341e-01, -3.5459950e+00, -1.7320968e+00,\n",
       "          1.9397168e+00, -6.3734710e-01,  2.3254216e+00, -1.3535864e+00,\n",
       "         -1.4451812e-01, -2.4297442e+00,  1.5498929e+00,  8.1969726e-01,\n",
       "          9.0982294e-01, -6.6116208e-01,  3.8905215e-01,  3.3855909e-01,\n",
       "         -7.5454485e-01, -1.0352553e+00, -2.5936973e+00,  1.2103225e+00,\n",
       "         -3.0236175e+00,  3.0580134e+00, -3.9140179e+00,  4.0223894e-01,\n",
       "          1.7356061e+00,  9.0976155e-01,  2.0956397e-02,  2.0190549e+00,\n",
       "          4.5332021e-01, -1.6634842e+00, -4.8180079e-01,  2.0414692e-01,\n",
       "         -5.9267312e-01, -1.4182589e+00, -9.7301149e-01,  5.1611459e-01,\n",
       "          2.0727324e+00,  2.0064230e+00, -7.5027935e-02, -1.1723986e+00,\n",
       "         -8.6943096e-01,  1.7028141e+00,  2.2190344e+00,  9.3605727e-01,\n",
       "         -2.3281140e+00, -1.1469719e+00, -7.4651584e-02, -1.3099817e+00,\n",
       "         -8.7831926e-01,  1.0963516e+00, -8.0801606e-01,  2.0202060e+00,\n",
       "         -2.8135295e+00, -1.5866054e+00,  1.7901597e+00, -1.9552002e+00,\n",
       "          8.5110778e-01, -4.1770697e+00,  5.4225093e-01, -1.9657525e+00,\n",
       "          2.0231185e+00, -1.8881788e+00,  5.9491843e-01,  8.3414179e-01,\n",
       "         -1.6628648e+00,  3.3324170e-01,  2.7011216e+00,  2.1156013e+00,\n",
       "          5.9008741e-01, -9.1122669e-01, -2.8129277e-01, -8.5378832e-01,\n",
       "          6.6145062e-01,  4.0669715e-01,  1.1161113e+00, -2.4905500e+00,\n",
       "          1.2948816e+00,  7.9231268e-01,  8.6634362e-01, -2.0348356e+00,\n",
       "          2.6159151e+00,  1.5450108e+00, -3.0899661e+00, -6.3572246e-01,\n",
       "         -2.5528045e+00,  3.4042710e-01, -2.1339917e+00, -3.5500580e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.predict(numpy.array([[30438]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, right? But let's not waste our time when the computer could tell us definitively and quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.predict(numpy.array([[wordvec.vocab['python'].index]]))[0][0] == wordvec['python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way to turn words into word vectors with Keras layers. Yes! Time to get some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB Dataset\n",
    "The [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) consists of movie reviews that have been marked as positive or negative. (There is also a built-in dataset of [Reuters newswires](https://keras.io/datasets/#reuters-newswire-topics-classification) that have been classified by topic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our labels consists of 0 or 1, which makes sense for positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1]\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:9])\n",
    "print(max(y_train))\n",
    "print(min(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But x is a bit more trouble. The words have already been converted to numbers -- numbers that have nothing to do with the word embeddings we spent time learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the help page for imdb, it appears there is a way to get the word back. Phew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module keras.datasets.imdb in keras.datasets:\n",
      "\n",
      "NAME\n",
      "    keras.datasets.imdb\n",
      "\n",
      "FUNCTIONS\n",
      "    get_word_index(path='imdb_word_index.json')\n",
      "        Retrieves the dictionary mapping word indices back to words.\n",
      "        \n",
      "        # Arguments\n",
      "            path: where to cache the data (relative to `~/.keras/dataset`).\n",
      "        \n",
      "        # Returns\n",
      "            The word index dictionary.\n",
      "    \n",
      "    load_data(path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3, **kwargs)\n",
      "        Loads the IMDB dataset.\n",
      "        \n",
      "        # Arguments\n",
      "            path: where to cache the data (relative to `~/.keras/dataset`).\n",
      "            num_words: max number of words to include. Words are ranked\n",
      "                by how often they occur (in the training set) and only\n",
      "                the most frequent words are kept\n",
      "            skip_top: skip the top N most frequently occurring words\n",
      "                (which may not be informative).\n",
      "            maxlen: sequences longer than this will be filtered out.\n",
      "            seed: random seed for sample shuffling.\n",
      "            start_char: The start of a sequence will be marked with this character.\n",
      "                Set to 1 because 0 is usually the padding character.\n",
      "            oov_char: words that were cut out because of the `num_words`\n",
      "                or `skip_top` limit will be replaced with this character.\n",
      "            index_from: index actual words with this index and higher.\n",
      "        \n",
      "        # Returns\n",
      "            Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
      "        \n",
      "        # Raises\n",
      "            ValueError: in case `maxlen` is so low\n",
      "                that no input sequence could be kept.\n",
      "        \n",
      "        Note that the 'out of vocabulary' character is only used for\n",
      "        words that were present in the training set but are not included\n",
      "        because they're not making the `num_words` cut here.\n",
      "        Words that were not seen in the training set but are in the test set\n",
      "        have simply been skipped.\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "\n",
      "FILE\n",
      "    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/datasets/imdb.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_offset = 3\n",
    "imdb_map = dict((index + imdb_offset, word) for (word, index) in imdb.get_word_index().items())\n",
    "imdb_map[0] = 'PADDING'\n",
    "imdb_map[1] = 'START'\n",
    "imdb_map[2] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knowledge about the initial indices came from [this stack overflow post](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset) after I got gibberish when I tried to translate the first review, below. It looks coherent now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([imdb_map[word_index] for word_index in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our IMDB word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [['PADDING'] + [imdb_map[word_index] for word_index in review] for review in x_train]\n",
    "test_sentences = [['PADDING'] + [imdb_map[word_index] for word_index in review] for review in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_wv_model = word2vec.Word2Vec(train_sentences + test_sentences + ['UNKNOWN'], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_wordvec = imdb_wv_model.wv\n",
    "del imdb_wv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dataset\n",
    "For this exercise, we're going to keep all inputs the same length (we'll see how to do variable-length later). This means we need to choose a maximum length for the review, cutting off longer ones and adding padding to shorter ones. What should we make the length? Let's understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest review: 2697 Shortest review: 70\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(review) for review in x_train + x_test]\n",
    "print('Longest review: {} Shortest review: {}'.format(max(lengths), min(lengths)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2697 words! Wow. Well, let's see how many reviews would get cut off at a particular cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8485 reviews out of 25000 are over 500.\n"
     ]
    }
   ],
   "source": [
    "cutoff = 500\n",
    "print('{} reviews out of {} are over {}.'.format(\n",
    "    sum([1 for length in lengths if length > cutoff]), \n",
    "    len(lengths), \n",
    "    cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train_padded = sequence.pad_sequences(x_train, maxlen=cutoff)\n",
    "x_test_padded = sequence.pad_sequences(x_test, maxlen=cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification without using the pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vector_model = Sequential()\n",
    "no_vector_model.add(Embedding(input_dim=len(imdb_map), output_dim=100, input_length=cutoff))\n",
    "no_vector_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "no_vector_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "no_vector_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "no_vector_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "no_vector_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "no_vector_model.add(Flatten())\n",
    "no_vector_model.add(Dense(units=128, activation='relu'))\n",
    "no_vector_model.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "no_vector_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. __This takes awhile. You might not want to re-run it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 226s 9ms/step - loss: 0.4238 - binary_accuracy: 0.7667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x111e2cef0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_vector_model.fit(x_train_padded, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the model. __This takes awhile. You might not want to re-run it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 68s 3ms/step\n",
      "loss: 0.3622105271434784 accuracy: 0.848\n"
     ]
    }
   ],
   "source": [
    "no_vector_scores = no_vector_model.evaluate(x_test_padded, y_test)\n",
    "print('loss: {} accuracy: {}'.format(*no_vector_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Use the word vectors in a full model\n",
    "Using the knowledge about how the imdb dataset and the keras embedding layer represent words, as detailed above, change the model in the three cells above this one to use the word vectors rather than an embedding that keras learns as it goes along.\n",
    "\n",
    "You'll need to swap out the embedding layer and feed in different training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions (Optional)\n",
    "1. How does the performance of the two models (with and without trained word vectors) compare?\n",
    "2. Why use one-dimensional convolutional layers for this problem? How do they help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
